{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "-0.09624052047729492\n",
      "-0.08042263984680176\n",
      "-0.07466483116149902\n",
      "-0.10356855392456055\n",
      "-0.10634970664978027\n",
      "-0.09688758850097656\n",
      "-0.1005105972290039\n",
      "-0.10463261604309082\n",
      "-0.10004711151123047\n",
      "-0.10182332992553711\n",
      "-0.0933375358581543\n",
      "-0.10653805732727051\n",
      "-0.09725022315979004\n",
      "-0.10520386695861816\n",
      "-0.08891844749450684\n",
      "-0.10292935371398926\n",
      "-0.10332226753234863\n",
      "-0.09438848495483398\n",
      "-0.0967566967010498\n",
      "-0.09603571891784668\n",
      "-0.09934449195861816\n",
      "-0.10131192207336426\n",
      "-0.09852313995361328\n",
      "-0.09572243690490723\n",
      "-0.09847307205200195\n",
      "-0.10033893585205078\n",
      "-0.09519791603088379\n",
      "-0.10734987258911133\n",
      "-0.0988607406616211\n",
      "-0.10141491889953613\n",
      "-0.10067033767700195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-667496394c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mupdate_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mupdate_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mupdate_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mupdate_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# print('epoch: %d || loss_1: %f' % (step, update_loss.data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Wenbo Yu\n",
    "This project is used to do regression of sensor data collected by vehicles.\n",
    "Sensors are divided in to control sensors which are X, and dependent sensors which are Y.\n",
    "The network used is LSTM. Besides, CNN and NN examples are provided.\n",
    "The script could be directly ran with the provided picke data.\n",
    "\n",
    "Version information:\n",
    "PyTorch = 1.4.0\n",
    "'''\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Pytorch LSTM network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, batch_size=1, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        # Define output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def init_hidden(self): # Not required\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda(), torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).cuda())\n",
    "\n",
    "    def point_grad_to(self, target):\n",
    "        '''\n",
    "        #### This function is important for meta learning. If other model is used, this one could be directly copied into other models ###\n",
    "        Set .grad attribute of each parameter to be proportional\n",
    "        to the difference between self and target\n",
    "        '''\n",
    "        for p, target_p in zip(self.parameters(), target.parameters()):\n",
    "            if p.grad is None:\n",
    "                # if self.is_cuda():\n",
    "                p.grad = Variable(torch.zeros(p.size())).cuda()\n",
    "                # else:\n",
    "                #     p.grad = Variable(torch.zeros(p.size()))\n",
    "            p.grad.data.zero_()  # not sure this is required\n",
    "            p.grad.data.add_(p.data - target_p.data)\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "        # shape of input  (seq_length, batch, input_dim)\n",
    "        # shape of output (seq_length, batch, hidden_dim)\n",
    "        lstm_out, hidden_state = self.lstm(input)\n",
    "        self.hidden = hidden_state\n",
    "        y_pred = self.linear(lstm_out)\n",
    "        y_pred = self.tanh(y_pred)\n",
    "        return y_pred, hidden_state\n",
    "\n",
    "###### Example of Convolutional network ######\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.l1 = nn.Sequential(\n",
    "#             nn.Linear(35, 256),\n",
    "#             # nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "#             )\n",
    "#         self.l2 = nn.Sequential(\n",
    "#             nn.Linear(256, 256),\n",
    "#             # nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "#             )\n",
    "#         self.l3 = nn.Sequential(\n",
    "#             nn.Linear(256, 64),\n",
    "#             # nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "#             )\n",
    "#         self.l4 = nn.Sequential(\n",
    "#             nn.Linear(64, 1),\n",
    "#             nn.Tanh()\n",
    "#             )\n",
    "#\n",
    "#     def forward(self, input):\n",
    "#         x = self.l1(input)\n",
    "#         x = self.l2(x)\n",
    "#         x = self.l3(x)\n",
    "#         output = self.l4(x)\n",
    "#         return output\n",
    "\n",
    "###### Example of Linear network ######\n",
    "# class NN(nn.Module):\n",
    "#     def __init__(self, features):\n",
    "#         super(NN, self).__init__()\n",
    "#         self.n = 3000\n",
    "#         self.features = features\n",
    "#         self.l1 = nn.Sequential(\n",
    "#             nn.Linear(self.features, self.n),\n",
    "#             # nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "#         )\n",
    "#         self.l2 = nn.Sequential(\n",
    "#             nn.Linear(self.n, self.n),\n",
    "#             # nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "#             )\n",
    "#         self.l3 = nn.Sequential(\n",
    "#             nn.Linear(self.n, 100),\n",
    "#             # nn.BatchNorm1d(256),\n",
    "#             nn.LeakyReLU(0.01),\n",
    "#         )\n",
    "#         self.l4 = nn.Sequential(\n",
    "#             nn.Linear(100, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = self.l1(input)\n",
    "#         x = self.l2(x)\n",
    "#         x = self.l3(x)\n",
    "#         output = self.l4(x)\n",
    "#         return output\n",
    "\n",
    "\n",
    "models = [607, 612] # Also refer to tasks/classes, indicating how many sources of data are used\n",
    "# models = [607, 611, 612, 613, 614, 615, 616, 617, 618, 619]\n",
    "sensor = 14 # The selected target sensor\n",
    "traininig_size = 1000 # Number of training data points for eacg task\n",
    "test_size = 200\n",
    "chosen_size = traininig_size + test_size\n",
    "data_path = 'meta_pickle'\n",
    "X = {}\n",
    "Y = {}\n",
    "flag = 0\n",
    "\n",
    "# Built training and test sets\n",
    "for model in models:\n",
    "    with open(os.path.join(data_path, '%d.pkl' % model), 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "        chosen_idx = random.sample([item for item in range(all_data.shape[0])], chosen_size)\n",
    "        chosen_data = all_data[chosen_idx, :, :]\n",
    "        # Put np data to tensor\n",
    "        all_data = torch.from_numpy(chosen_data).to(dtype=torch.float32)\n",
    "        x = all_data[:, :, :14]\n",
    "        y = all_data[:, :, 14:]\n",
    "    X[model] = {}\n",
    "    Y[model] = {}\n",
    "    X[model]['training'] = x[:traininig_size, :, :]\n",
    "    Y[model]['training'] = y[:traininig_size, :, :]\n",
    "    if flag == 0:\n",
    "        x_test = x[traininig_size:, :, :]\n",
    "        y_test = y[traininig_size:, :, :]\n",
    "        flag = 1\n",
    "    else:\n",
    "        x_test = torch.cat([x_test, x[traininig_size:, :, :]], dim=0)\n",
    "        y_test = torch.cat([y_test, y[traininig_size:, :, :]], dim=0)\n",
    "\n",
    "# Start training\n",
    "# Set hyperparameters\n",
    "task_num = len(models)\n",
    "Step = 10000\n",
    "batch_size = 5\n",
    "meta_lr = 1e-5\n",
    "update_lr = 1e-5\n",
    "update_step = 5\n",
    "input_dim = 14\n",
    "hidden_dim = 500\n",
    "output_dim = 16\n",
    "net = LSTM(input_dim, hidden_dim, output_dim).cuda()\n",
    "\n",
    "# Optimizers\n",
    "meta_optimizer = torch.optim.Adam(net.parameters(), lr=meta_lr)\n",
    "L1Loss = torch.nn.MSELoss()\n",
    "\n",
    "x_test = x_test.cuda().transpose(0, 1) # Required for LSTM network, the input dimension is [length, batch, feature]\n",
    "y_test = y_test.cuda()\n",
    "\n",
    "print('Start training')\n",
    "for step in range(Step):\n",
    "    meta_loss_total = 0\n",
    "    task_order = [i for i in range(len(models))]\n",
    "    random.shuffle(task_order)\n",
    "    for i in task_order:\n",
    "        # Choose data\n",
    "        vehicle = models[i]\n",
    "        num = X[vehicle]['training'].shape[0]\n",
    "        batch_list = [item for item in range(traininig_size)]\n",
    "        batch_content = random.sample(batch_list, batch_size)\n",
    "        x_batch = X[vehicle]['training'][batch_content, :].cuda().transpose(0, 1)\n",
    "        y_batch = Y[vehicle]['training'][batch_content, :].cuda()\n",
    "\n",
    "        # Initialize network. This network 'net_i' is used to train several steps, and the learnt parameters are used to update 'net'\n",
    "        net_i = LSTM(input_dim, hidden_dim, output_dim).cuda()\n",
    "        hidden_state = net_i.init_hidden()\n",
    "        net_i.load_state_dict(net.state_dict())\n",
    "        update_optimizer = torch.optim.Adam(net_i.parameters(), lr=update_lr)\n",
    "        \n",
    "\n",
    "        # Update process\n",
    "        start = time.time()\n",
    "        for j in range(update_step):\n",
    "            y_pred, hidden_state_i = net_i(x_batch, hidden_state)\n",
    "            y_pred = y_pred.transpose(0, 1)\n",
    "            update_loss = L1Loss(y_pred, y_batch)\n",
    "            update_optimizer.zero_grad()\n",
    "            update_loss.backward(retain_graph=True)\n",
    "            update_optimizer.step()\n",
    "            # print('epoch: %d || loss_1: %f' % (step, update_loss.data))\n",
    "        net.point_grad_to(net_i)\n",
    "        meta_optimizer.step()\n",
    "        end = time.time()\n",
    "        print(start-end)\n",
    "    # 1st order update\n",
    "    y_pred, hidden_state_i = net(x_test, hidden_state)\n",
    "    y_pred = y_pred.transpose(0, 1)\n",
    "    update_loss = L1Loss(y_pred, y_test)\n",
    "    \n",
    "#     print('epoch: %d || loss_1: %f' % (step, update_loss.data))\n",
    "#         # file = open('out' + '/log.txt', 'a')\n",
    "#         # file.write('task: %d || epoch: %d || loss_1: %f\\n' % (i, step, meta_loss.data))\n",
    "#         # file.close()\n",
    "#     if step % 100 == 0:\n",
    "#         file = open('out2/log.txt', 'a')\n",
    "#         file.write('epoch: %d || loss_1: %f\\n' % (step, update_loss.data))\n",
    "#         file.close()\n",
    "#     if step > 2000 and step % 200 == 0:\n",
    "#         # Save weights\n",
    "#         weight_name = 'out2/model_%d.ckpt' % (step)\n",
    "#         torch.save(net.state_dict(), weight_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
